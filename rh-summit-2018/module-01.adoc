== Deploying an Apache Kafka cluster with Strimzi

Let's get started by deploying an Apache Kafka cluster on OpenShift.
For this, we will use http://strimzi.io/[Strimzi], an open-source project that simplifies the process of deploying and managing Apache Kafka clusters on Kubernetes and OpenShift.

=== How Strimzi works

You can run an Apache Kafka cluster on Kubernetes, and by extension, on OpenShift, in a variety of ways, not all being equal in terms of ease of use and maintenance.

For example, you can deploy the cluster manually as a stateful set.
While this can get you past the initial hurdle of starting the cluster, soon you have to start performing more complex tasks such as changing cluster topology, modifying configuration, or administering topics.
These tasks typically require direct access to the cluster nodes and can easily become cumbersome.

==== Kubernetes Operators ====

Strimzi simplifies these tasks by using a declarative approach to cluster and topic management, implemented as https://coreos.com/operators/[Kubernetes Operators].
A Kubernetes Operator is an application-specific controller that extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user.
It builds upon the basic Kubernetes resource and controller concepts but includes domain or application-specific knowledge to automate common tasks.

Instead of relying on direct deployment and management of Zookeeper and Kafka clusters, Strimzi consists of a couple of these domain-specific controllers that monitor the state of the cluster, making adjustments in accordance to a desired state read from dedicated ConfigMaps.

For creating an Apache Kafka cluster, for instance, you need to create a ConfigMap that describes the properties of the cluster, and the *_cluster controller_* will deploy the cluster for you.
If you need to change the state of the cluster, for example for changing properties or for adding new instances, all you have to do is to modify the ConfigMap and the changes will be rolled out accordingly.

Topic management works in a similar fashion: for creating and modifying topics, you only need to create and edit a set of ConfigMaps and the *_topic controller_* will do the work for you.

You will do all this as part of the first lab.

=== Installing Strimzi

First, connect via SSH to the workstation.
The keys for connecting remotely to the workstation are already provisioned on your machine.

[source, sh]
$ ssh workstation-<GUID>.rhpds.opentlc.com

Download Strimzi and extract it

[source, sh]
$ wget https://github.com/strimzi/strimzi/releases/download/0.3.0/strimzi-0.3.0.tar.gz
$ tar xzvf strimzi-0.3.0.tar.gz

Enter the `strimzi-0.3.0` directory.

[source, sh]
$ cd strimzi-0.3.0

Log in as administrator to install Strimzi
(use the password provided by the lab instructors):

[source,sh]
$ oc login -u admin https://master.example.com:8443

Note that the workstation is on the same virtual network as the OpenShift cluster.
The URL `master.example.com` is only available to use from the workstation.

Next, create a new project for the lab:

[source, sh]
$ oc new-project l1099-kafka

You can see that it is empty:

[source, sh]
$ oc get pods
No resources found.

Install the cluster controller:

[source, sh]
$ oc create -f examples/install/cluster-controller

You should see a few resources being created:

[source, sh]
serviceaccount "strimzi-cluster-controller" created
role "strimzi-cluster-controller-role" created
rolebinding "strimzi-cluster-controller-binding" created
deployment "strimzi-cluster-controller" created

The service account `strimzi-cluster-controller` is granted permission to access various resources in the project.
This allows it to read the config maps containing the cluster configuration that we will create later in the process.

Now, make sure that the cluster controller is deployed.

[source,sh]
$ oc get pods

The command output should be similar to:

[source,sh]
NAME                                          READY     STATUS    RESTARTS   AGE
strimzi-cluster-controller-2044197322-lzrvr   1/1       Running   0          3m

Next, install the Strimzi templates.
The Cluster Controller related templates contain predefined config maps for easily deploying clusters (for Kafka Connect as well).

[source, sh]
$ oc create -f examples/templates/cluster-controller
template "strimzi-connect-s2i" created
template "strimzi-connect" created
template "strimzi-ephemeral" created
template "strimzi-persistent" created

The Topic Controller related templates contain predefined config map for easily creating a new Kafka topic.

[source,sh]
$ oc create -f examples/templates/topic-controller
template "strimzi-topic" created

Now you can deploy a Kafka cluster by creating a config map.
For this lab, we will use a template, but you can create and deploy a regular config map as well.

[source,sh]
$ oc new-app strimzi-ephemeral

The expected output should be similar to:
[source,sh]
-----
--> Deploying template "l1099-kafka/strimzi-ephemeral" to project l1099-kafka

     Apache Kafka (Ephemeral storage)
     ---------
     This template installs Apache Zookeeper and Apache Kafka clusters. For more information about using this template see http://strimzi.io

     WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing."

     Kafka cluster my-cluster is being deployed. Use 'my-cluster:9092' as bootstrap server in your application

     * With parameters:
        * Name of the cluster=my-cluster
        * Number of Zookeper cluster nodes (odd number of nodes is recomended)=1
        * Number of Kafka cluster nodes=3
        * Repository Name=strimzi
        * Kafka image Name=kafka
        * Kafka image tag=0.3.0
        * Zookeeper image Name=zookeeper
        * Zookeeper image tag=0.3.0
        * Zookeeper healthcheck initial delay=15
        * Zookeeper healthcheck timeout=5
        * Kafka healthcheck initial delay=15
        * Kafka healthcheck timeout=5
        * Default replication factor=1
        * Offsets replication factor=3
        * Transaction state replication factor=3

--> Creating resources ...
    configmap "my-cluster" created
--> Success
    Run 'oc status' to view your app.
-----

The key resource is the `my-cluster` config map.

Let's see what it contains.

[source,sh]
---------
$ oc get cm my-cluster -o yaml
apiVersion: v1
data:
  KAFKA_DEFAULT_REPLICATION_FACTOR: "1"
  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "3"
  KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "3"
  kafka-healthcheck-delay: "15"
  kafka-healthcheck-timeout: "5"
  kafka-image: strimzi/kafka:0.3.0
  kafka-metrics-config: |-
    {
      "lowercaseOutputName": true,
      "rules": [
          {
            "pattern": "kafka.server<type=(.+), name=(.+)PerSec\\w*><>Count",
            "name": "kafka_server_$1_$2_total"
          },
          {
            "pattern": "kafka.server<type=(.+), name=(.+)PerSec\\w*, topic=(.+)><>Count",
            "name": "kafka_server_$1_$2_total",
            "labels":
            {
              "topic": "$3"
            }
          }
      ]
    }
  kafka-nodes: "3"
  kafka-storage: '{ "type": "ephemeral" }'
  topic-controller-config: '{ }'
  zookeeper-healthcheck-delay: "15"
  zookeeper-healthcheck-timeout: "5"
  zookeeper-image: strimzi/zookeeper::0.3.0
  zookeeper-metrics-config: |-
    {
      "lowercaseOutputName": true
    }
  zookeeper-nodes: "1"
  zookeeper-storage: '{ "type": "ephemeral" }'
kind: ConfigMap
metadata:
  creationTimestamp: 2018-04-18T08:06:50Z
  labels:
    app: strimzi-ephemeral
    strimzi.io/kind: cluster
    strimzi.io/type: kafka
  name: my-cluster
  namespace: myproject
  resourceVersion: "1837"
  selfLink: /api/v1/namespaces/myproject/configmaps/my-cluster
  uid: 72f8e336-42df-11e8-9953-54ee758f9350
---------

The properties of the map control the cluster configuration.
Notice the `kafka-nodes` and `zookeeper-nodes` properties, with values of 3 and 1, respectively.
This deployment has one Zookeeper node and three Kafka brokers.

Visualize the running pods:

[source,sh]
$ oc get pods

You might need to run the command a few times, with different results, as the pods are spinning up, but you should see the results stabilizing as:

[source,sh]
$ oc get pods
NAME                                           READY     STATUS    RESTARTS   AGE
my-cluster-kafka-0                             1/1       Running   0          2m
my-cluster-kafka-1                             1/1       Running   0          2m
my-cluster-kafka-2                             1/1       Running   0          2m
my-cluster-topic-controller-1422164134-f9n8r   1/1       Running   0          1m
my-cluster-zookeeper-0                         1/1       Running   0          3m
strimzi-cluster-controller-2044197322-lzrvr    1/1       Running   0          11m

In addition to the `cluster controller` created previously, notice a few more deployments:

* the `topic controller` is now deployed as well - you can deploy it independently, but the Strimzi template deploys it out of the box;
* one Zookeeper node
* three Kafka brokers

Also, notice that the Zookeeper ensemble and the Kafka cluster are deployed as stateful sets.

=== Monitoring with Prometheus and Grafana

By default, Strimzi provides the Kafka brokers and the Zookeeper nodes with a Prometheus JMX exporter agent which is running in order to export metrics.
These metrics can be read and processed by a Prometheus server in order to monitoring the cluster.
For building a graphical dashboard with such information, it's possible to use Grafana.

==== Prometheus

The Prometheus service pod runs with `prometheus-server` service account and it needs to have access to the API server to get the pod list and for allowing that, the following command is needed.

[source,sh]
$ export NAMESPACE=l1099-kafka
$ oc create sa prometheus-server
$ oc adm policy add-cluster-role-to-user cluster-reader system:serviceaccount:${NAMESPACE}:prometheus-server

Finally, create the Prometheus service by running.

[source,sh]
$ oc create -f https://raw.githubusercontent.com/strimzi/strimzi/master/metrics/examples/prometheus/kubernetes.yaml

==== Grafana

The Grafana server is really useful to get a visualisation of the Prometheus metrics.

To deploy Grafana on OpenShift, the following commands should be executed:

[source,sh]
$ oc create -f https://raw.githubusercontent.com/strimzi/strimzi/master/metrics/examples/grafana/kubernetes.yaml

You can access the Grafana UI after running:

[source,sh]
$ oc expose svc/grafana

The hostname of the service is available in the OpenShift console, or can be retrieved via CLI:

[source,sh]
$ oc get routes grafana -o=jsonpath='{.spec.host}{"\n"}'

Note the output, which should be in the format `grafana-l1099-kafka.apps-<GUID>.generic.opentlc.com/` and access the Grafana UI at that URL in your browser.
Now we can set up the Prometheus data source and the Kafka dashboard.

Access to the Grafana UI using `admin/admin` credentials.

image::grafana_login.png[grafana login]

Click on the "Add data source" button from the Grafana home in order to add Prometheus as data source.

image::grafana_home.png[grafana home]

Fill in the information about the Prometheus data source, specifying a name and "Prometheus" as type.
In the URL field, use `http://prometheus:9090` as the URL to the Prometheus server.
After "Add" is clicked, Grafana will test the connection to the data source.

image::grafana_prometheus_data_source.png[grafana prometheus data source]

From the top left menu, click on "Dashboards" and then "Import" to open the "Import Dashboard" window.
Open a browser tab and navigate to `https://raw.githubusercontent.com/strimzi/strimzi/0.3.0/metrics/examples/grafana/kafka-dashboard.json`.
You should see JSON content as response.
Copy and paste it in the appropriate field in the form.

image::grafana_import_dashboard.png[grafana import dashboard]

After importing the dashboard, the Grafana home should show with some initial metrics about CPU and JVM memory usage.
When the Kafka cluster is used (creating topics and exchanging messages) the other metrics, like messages in and bytes in/out per topic, will be shown.

image::grafana_kafka_dashboard.png[grafana kafka dashboard]

=== Handling cluster and topics

Before starting to develop data streaming applications and running them, let's see how it's possible to handle the Kafka cluster itself and the topics.

==== Updating Kafka cluster

Starting from the current Kafka cluster with 3 brokers, we want to add two more.
In order to do that, the related `my-cluster` config map needs to be updated using the "edit" command provided by the `oc` tool.

[source,sh]
$ oc edit cm my-cluster

It opens the default editor that we can use in order to change the value of the `kafka-nodes` field from 3 to 5.
After saving the file, the Cluster Controller detects the update and starts a two new broker Pods; it's just a simple scale-up operation.
You can see this by visualizing the pods again:

[source,sh]
$ oc get pods
NAME                                           READY     STATUS    RESTARTS   AGE
my-cluster-kafka-0                             1/1       Running   0          9m
my-cluster-kafka-1                             1/1       Running   0          9m
my-cluster-kafka-2                             1/1       Running   0          9m
my-cluster-kafka-3                             1/1       Running   0          1m
my-cluster-kafka-4                             1/1       Running   0          1m
my-cluster-topic-controller-1422164134-f9n8r   1/1       Running   0          8m
my-cluster-zookeeper-0                         1/1       Running   0          10m
strimzi-cluster-controller-2044197322-lzrvr    1/1       Running   0          18m

Notice the new pods `my-cluster-kafka-3` and `my-cluster-kafka-4`.
For the rest of the lab, we only need three Kafka brokers, so we recommend to size down the cluster to 3 nodes, by editing the config map again:

[source,sh]
$ oc edit cm my-cluster

Set the `kafka-nodes` field back to 3 and check that the two additional pods have been shut down.

[source,sh]
$ oc get pods
NAME                                           READY     STATUS    RESTARTS   AGE
my-cluster-kafka-0                             1/1       Running   0          11m
my-cluster-kafka-1                             1/1       Running   0          11m
my-cluster-kafka-2                             1/1       Running   0          11m
my-cluster-topic-controller-1422164134-f9n8r   1/1       Running   0          10m
my-cluster-zookeeper-0                         1/1       Running   0          12m
strimzi-cluster-controller-2044197322-lzrvr    1/1       Running   0          20m



Now we want to do something more interesting like changing a Kafka broker configuration parameter, for example the `KAFKA_DEFAULT_REPLICATION_FACTOR` one modifying its value from 1 to 2.

Before doing that let's check that the default replication factor is 1 getting the log from one of the running brokers.

[source,sh]
$ oc logs my-cluster-kafka-0 | grep default.replication.factor
default.replication.factor=1
	default.replication.factor = 1

In the same way as before you can use the "edit" command and updating that value in the default editor.

[source,sh]
$ oc edit cm my-cluster

This kind of update is much more complex because changing the Kafka broker configuration we want all the running brokers to be updated so it means that each broker needs to be restarted in order to get the new configuration.
In this case, detecting the config map update, the Cluster Controller starts a "rolling update" and each broker Pod is killed one by one and then restarted with the new configuration.

When the "rolling update" is finished we can check that the default replication factor is changed to 2.

[source,sh]
$ oc logs my-cluster-kafka-0 | grep default.replication.factor
default.replication.factor=2
	default.replication.factor = 2

==== Handling topics

It's possible to create a topic just creating a regular config map but for this lab we are going to use the related template.

[source,sh]
$ oc new-app strimzi-topic

The expected output should be similar to:
[source,sh]
-----
--> Deploying template "myproject/strimzi-topic" to project myproject

     Apache Kafka Topic
     ---------
     This template creates a "Topic ConfigMap". Used in conjunction with the Strimzi topic controller this will create a corresponding topic in a Strimzi Kafka cluster. For more information about using this template see http://strimzi.io

     * With parameters:
        * Name of the Kafka cluster=my-cluster
        * Name of the ConfigMap=my-topic
        * Name of the topic=my-topic
        * Number of partitions=1
        * Number of replicas=1
        * Topic config={}

--> Creating resources ...
    configmap "my-topic" created
--> Success
    Run 'oc status' to view your app.
-----

The key resource is the `my-topic` config map.

Let's see what it contains.

[source,sh]
---------
$ oc get cm my-topic -o yaml
apiVersion: v1
data:
  config: '{}'
  name: my-topic
  partitions: "1"
  replicas: "1"
kind: ConfigMap
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2018-04-26T09:34:37Z
  labels:
    app: strimzi-topic
    strimzi.io/cluster: my-cluster
    strimzi.io/kind: topic
  name: my-topic
  namespace: myproject
  resourceVersion: "10543"
  selfLink: /api/v1/namespaces/myproject/configmaps/my-topic
  uid: 092b92f2-4935-11e8-82f5-54ee758f9350
---------

The properties of the map control the topic configuration.

In order to check that the Topic Controller has detected the new config map and created a related topic in the Kafka cluster, we can run the official `kafka-topics.sh` tool on one of the brokers.

[source,sh]
$ oc exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh --zookeeper my-cluster-zookeeper:2181 --describe
Topic:my-topic	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: my-topic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0

Let's increase the partitions number now.
It's possible just updating the related config map and changing the `partitions` data field from 1 to 3, for example using the "edit" command provided by the `oc` tool.

[source,sh]
$ oc edit cm my-topic

The Topic Controller detects this update and updates the related Kafka topic accordingly.
We can check that describing the topic one more time.

[source,sh]
$ oc exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh --zookeeper my-cluster-zookeeper:2181 --describe
Topic:my-topic	PartitionCount:3	ReplicationFactor:1	Configs:
	Topic: my-topic	Partition: 0	Leader: 1	Replicas: 1	Isr: 1
	Topic: my-topic	Partition: 1	Leader: 2	Replicas: 2	Isr: 2
	Topic: my-topic	Partition: 2	Leader: 3	Replicas: 3	Isr: 3

Finally, a topic can be deleted just deleting the related config map.

[source,sh]
$ oc delete cm my-topic
configmap "my-topic" deleted

The Topic Controller detects the deletion and deletes the related Kafka topic from the cluster.
We can check that listing the available topics.

[source,sh]
$ oc exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh --zookeeper my-cluster-zookeeper:2181 --list

This time the output should be empty.

Now your Kafka cluster is running and ready to go.
Let's build some applications!
