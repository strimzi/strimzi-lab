== Deploying an Apache Kafka cluster with Strimzi

Let's get started by deploying an Apache Kafka cluster on OpenShift.
For this, we will use http://strimzi.io/[Strimzi], an open-source project that simplifies the process of deploying and managing Apache Kafka clusters on Kubernetes and OpenShift.

=== How Strimzi works

You can run an Apache Kafka cluster on Kubernetes, and by extension, on OpenShift, in a variety of ways, not all being equal in terms of ease of use and maintenance.

For example, you can deploy the cluster manually as a stateful set.
While this can get you past the initial hurdle of starting the cluster, soon you have to start performing more complex tasks such as changing cluster topology, modifying configuration, or administering topics.
These tasks typically require direct access to the cluster nodes and can easily become cumbersome.

Strimzi simplifies these tasks by using a declarative approach to cluster and topic management, based on the controller pattern.
Instead of relying on direct deployment and management of clusters, Strimzi consists of a couple of controllers that monitor the state of the cluster, making adjustments in accordance to a desired state read from dedicated ConfigMaps.

For creating an Apache Kafka cluster, for instance, you need to create a ConfigMap that describes the properties of the cluster, and the *_cluster controller_* will deploy the cluster for you.
If you need to change the state of the cluster, for example for changing properties or for adding new instances, all you have to do is to modify the ConfigMap and the changes will be rolled out accordingly.

Topic management works in a similar fashion: for creating and modifying topics, you only need to create and edit a set of ConfigMaps and the *_topic controller_* will do the work for you.

You will do all this as part of the first lab.

=== Installing Strimzi

First, connect via SSH to the workstation.
The keys for connecting remotely to the workstation are already provisioned on your machine.

[source, sh]
$ ssh cloud-user@workstation-<GUID>.generic.opentlc.com

Strimzi is already downloaded on your machine and available in the `strimzi` directory.

[source, sh]
$ cd strimzi

Log in as administrator to install Strimzi:
# We will need to provide a better password for this, but I will not put it in GitHub for now

[source,sh]
$ oc login -u admin https://master.example.com:8443

Note that the workstation is on the same virtual network as the OpenShift cluster.
The URL `master.example.com` is only available to use from the workstation.

Next, create a new project for the lab:

[source, sh]
$ oc new-project l1099-kafka

You can see that it is empty:

[source, sh]
$ oc get pods
No resources found.

Install the cluster controller:

[source, sh]
$ oc create -f examples/install/cluster-controller

You should see a few resources being created:

[source, sh]
serviceaccount "strimzi-cluster-controller" created
role "strimzi-cluster-controller-role" created
rolebinding "strimzi-cluster-controller-binding" created
deployment "strimzi-cluster-controller" created

The service account `strimzi-cluster-controller` is granted permission to access various resources in the project.
This allows it to read the config maps containing the cluster configuration that we will create later in the process.

Now, make sure that the cluster controller is deployed.

[source,sh]
$ oc get pods

The command output should be similar to:

[source,sh]
NAME                                          READY     STATUS    RESTARTS   AGE
strimzi-cluster-controller-2044197322-lzrvr   1/1       Running   0          3m

Next, install the Strimzi templates.
The templates contain predefined config maps for easily deploying clusters (for Kafka Connect as well).

[source, sh]
$ oc create -f examples/templates/cluster-controller
template "strimzi-connect-s2i" created
template "strimzi-connect" created
template "strimzi-ephemeral" created
template "strimzi-persistent" created

Now you can deploy a Kafka cluster by creating a config map.
For this lab, we will use a template, but you can create and deploy a regular config map as well.

[source,sh]
$ oc new-app strimzi-ephemeral

The expected output should be similar to:
[source,sh]
-----
--> Deploying template "l1099-kafka/strimzi-ephemeral" to project l1099-kafka

     Apache Kafka (Ephemeral storage)
     ---------
     This template installs Apache Zookeeper and Apache Kafka clusters. For more information about using this template see http://strimzi.io

     WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing."

     Kafka cluster my-cluster is being deployed. Use 'my-cluster:9092' as bootstrap server in your application

     * With parameters:
        * Name of the cluster=my-cluster
        * Number of Zookeper cluster nodes (odd number of nodes is recomended)=1
        * Number of Kafka cluster nodes=3
        * Repository Name=strimzi
        * Kafka image Name=kafka
        * Kafka image tag=0.2.0
        * Zookeeper image Name=zookeeper
        * Zookeeper image tag=0.2.0
        * Zookeeper healthcheck initial delay=15
        * Zookeeper healthcheck timeout=5
        * Kafka healthcheck initial delay=15
        * Kafka healthcheck timeout=5
        * Default replication factor=1
        * Offsets replication factor=3
        * Transaction state replication factor=3

--> Creating resources ...
    configmap "my-cluster" created
--> Success
    Run 'oc status' to view your app.
-----

The key resource is the `my-cluster` config map.

Let's see what it contains.

[source,sh]
---------
$ oc get cm my-cluster -o yaml
apiVersion: v1
data:
  KAFKA_DEFAULT_REPLICATION_FACTOR: "1"
  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "3"
  KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "3"
  kafka-healthcheck-delay: "15"
  kafka-healthcheck-timeout: "5"
  kafka-image: strimzi/kafka:0.2.0
  kafka-metrics-config: |-
    {
      "lowercaseOutputName": true,
      "rules": [
          {
            "pattern": "kafka.server<type=(.+), name=(.+)PerSec\\w*><>Count",
            "name": "kafka_server_$1_$2_total"
          },
          {
            "pattern": "kafka.server<type=(.+), name=(.+)PerSec\\w*, topic=(.+)><>Count",
            "name": "kafka_server_$1_$2_total",
            "labels":
            {
              "topic": "$3"
            }
          }
      ]
    }
  kafka-nodes: "3"
  kafka-storage: '{ "type": "ephemeral" }'
  topic-controller-config: '{ }'
  zookeeper-healthcheck-delay: "15"
  zookeeper-healthcheck-timeout: "5"
  zookeeper-image: strimzi/zookeeper::0.2.0
  zookeeper-metrics-config: |-
    {
      "lowercaseOutputName": true
    }
  zookeeper-nodes: "1"
  zookeeper-storage: '{ "type": "ephemeral" }'
kind: ConfigMap
metadata:
  creationTimestamp: 2018-04-18T08:06:50Z
  labels:
    app: strimzi-ephemeral
    strimzi.io/kind: cluster
    strimzi.io/type: kafka
  name: my-cluster
  namespace: myproject
  resourceVersion: "1837"
  selfLink: /api/v1/namespaces/myproject/configmaps/my-cluster
  uid: 72f8e336-42df-11e8-9953-54ee758f9350
---------

The properties of the map control the cluster configuration.
Notice the `kafka-nodes` and `zookeeper-nodes` properties, with values of 3 and 1, respectively.
This deployment has one Zookeeper node and three Kafka brokers.

Visualize the running pods:

[source,sh]
$ oc get pods

You might need to run the command a few times, with different results, as the pods are spinning up, but you should see the results stabilizing as:

[source,sh]
$ oc get pods
NAME                                           READY     STATUS    RESTARTS   AGE
my-cluster-kafka-0                             1/1       Running   0          2m
my-cluster-kafka-1                             1/1       Running   0          2m
my-cluster-kafka-2                             1/1       Running   0          2m
my-cluster-topic-controller-1422164134-f9n8r   1/1       Running   0          1m
my-cluster-zookeeper-0                         1/1       Running   0          3m
strimzi-cluster-controller-2044197322-lzrvr    1/1       Running   0          11m

In addition to the `cluster controller` created previously, notice a few more deployments:

* the `topic controller` is now deployed as well - you can deploy it independently, but the Strimzi template deploys it out of the box;
* one Zookeeper node
* three Kafka brokers

Also, notice that the Zookeeper ensemble and the Kafka cluster are deployed as stateful sets.

=== Monitoring with Prometheus and Grafana

By default, Strimzi provides the Kafka brokers and the Zookeeper nodes with a Prometheus JMX exporter agent which is running in order to export metrics.
These metrics can be read and processed by a Prometheus server in order to monitoring the cluster.
For building a graphical dashboard with such information, it's possible to use Grafana.

==== Prometheus

The Prometheus service pod runs with `prometheus-server` service account and it needs to have access to the API server to get the pod list and for allowing that, the following command is needed.

[source,sh]
export NAMESPACE=l1099-kafka
oc create sa prometheus-server
oc adm policy add-cluster-role-to-user cluster-reader system:serviceaccount:${NAMESPACE}:prometheus-server

Finally, create the Prometheus service by running.

[source,sh]
oc create -f https://raw.githubusercontent.com/strimzi/strimzi/master/metrics/examples/prometheus/kubernetes.yaml

==== Grafana

The Grafana server is really useful to get a visualisation of the Prometheus metrics.

To deploy Grafana on OpenShift, the following commands should be executed:

[source,sh]
oc create -f https://raw.githubusercontent.com/strimzi/strimzi/master/metrics/examples/grafana/kubernetes.yaml

In order to visualize the exported metrics in Grafana, the simple dashboard `kafka-dashboard.json`` file is provided.
The Prometheus data source, and the above dashboard, can be set up in Grafana by following these steps.

Access to the Grafana UI using `admin/admin` credentials.

image::grafana_login.png[grafana login]

Click on the "Add data source" button from the Grafana home in order to add Prometheus as data source.

image::grafana_home.png[grafana home]

Fill in the information about the Prometheus data source, specifying a name and "Prometheus" as type.
In the URL field, the connection string to the Prometheus server `http://prometheus:9090`` should be specified.
After "Add" is clicked, Grafana will test the connection to the data source.

image::grafana_prometheus_data_source.png[grafana prometheus data source]

From the top left menu, click on "Dashboards" and then "Import" to open the "Import Dashboard" window where the provided `kafka-dashboard.json`` file can be imported or its content pasted.

image::grafana_import_dashboard.png[grafana import dashboard]

After importing the dashboard, the Grafana home should show with some initial metrics about CPU and JVM memory usage.
When the Kafka cluster is used (creating topics and exchanging messages) the other metrics, like messages in and bytes in/out per topic, will be shown.

image::grafana_kafka_dashboard.png[grafana kafka dashboard]

Now your Kafka cluster is running and ready to go.
Let's build some applications!
